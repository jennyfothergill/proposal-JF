\documentclass{article}
\title{Reading Notes}

\begin{document}
\title{}

today I got this repo set up. Next time, I'll reskim these papers:

MARTINI\cite{Marrink2007}

VOTCA\cite{Ruhle2009}

TraPPE\cite{Maerzke2011}

and read this: \cite{Chakraborty2018a} 

got busy with fancy-ing up the GIXStapose repo, helping rachel, office hours, and atom-typing with gaff-foyer with ryan and chris. Briefly skimmed the chakraborty paper-- they consider all possible mappings that a cg compound can have in order to systematically pick a cging scheme.
OK Chakraborty notes:
-exclude cg mapping schemes to those that maintain symmetry
-Zhang and Voth designed CG mapping operators by preserving "essential dynamics"
-Cao and Voth worked on centering functions COM vs center of charge. They found center of change was better for preserving intermolecular structure.
-Wagner generalize centering functions to arbitrary thermodynamic quantities
-Marrink grouped four heavy atoms and showed that this simple consistent scheme works.
CG models are validated by preservation of rdfs and velocity autocorrelation functions (vacfs)
mapping operator tree

Today I'm reading the VOTCA\cite{Ruhle2009} paper with the goal of answering these questions:
how do they choose a chromophore?

how do they calculate the transfer integral?

how do they do backmapping?

other notes:
CG methods--IBI, force matching, inverse MC. This paper goes over details of these and how they're implemented in votca. This wasn't really what I wanted to read about so I skimmed.
They use ESPResSo++ for quantum calculations
OK, this isn't the right paper to read about CT stuff or backmapping. Let's look at some of these [http://www.votca.org/citations/using-votca](http://www.votca.org/citations/using-votca)
Also I'm noticing in this list of papers--Marrink (cited by chakraborty) is here and there is a paper mentioning the importance of side-chains for charge-mobility! \cite{May2011}
Next time read about tie-chains \cite{May2011} and/or charge mobility \cite{Lukyanov2010}
Lukyanov--sounds a lot like morphct, gaussian density of states? I thought their (votca's) method was like morphct but now I'm not sure... It seems like they're using a simplification and not doing dft.

https://pubs.acs.org/doi/10.1021/ja104380c - 
rubrene record charge carrier $15 \frac{cm^{2}}{(V s)}$ single crystal by vapor deposition. typical for thin-film is $1 {cm^{2}}{(V s)}$

good discussion of some of the assumption of marcus theory. 
Drude model (electrons bounce off stationary nuclei like a pinball machine) works for perfect crystals at low temp.
At RT it doesn't work. Marcus theory assumes nuclear dynamics much slower (Borne-Oppenheimer) and electronic coupling is weak (what does this mean exactly?). Static disorder is based on the electronic DOS and the hopping rate between localized states. (As opposed to what?)
They start with single crystal structures from xray data. They calculate reorganization energies with b3lyp 6-311G(d,p) and transfer integrals with ZINDO. (check their SI)
How are they able to calculate direction-resolved transfer integrals?? very cool. What exactly do the transfer integral graphs mean? (Figure 2) Had a helpful chat with Chris about this and it helped me to think through the figures. The y-axis on the transfer integral is counts and the x-axis is energies--it makes sense that movement of an electron in different directions would require different energies and/or be more likely. Still not sure how to calculate it though. 

\cite{Vehoff2010a}

rubrene record charge carrier $15 \frac{cm^{2}}{V s}$ single crystal by vapor deposition. typical for thin-film is $1 \frac{cm^{2}}{V s}$

Today I'm digging around in more of the votca stuff to try and figure out how to run kmc there. They use ORCA too! Also their tutorials and documentation REALLY SUCK. [https://github.com/votca/xtp-tutorials](https://github.com/votca/xtp-tutorials) in particular I'm looking at LAMMPS/KMC\_Thiophene.
OK. either I am dumb or that tutorial sucks. I can tell that they're using orca and the inputs/oputputs the use but there's no explanation of what they're doing. It looks like they calculate the reorganization energy, but they don't define a chromophore--which makes sense for thiophene but what about when your molecule is ginormous?

\cite{Ruhle2011b} 

discusses charge transport in amorphous films of [tris-(8-hydroxyquinoline)aluminum](https://en.wikipedia.org/wiki/Tris(8-hydroxyquinolinato)aluminium). Oh Figure 1 is nice--it is a workflow of what can be done in votca.
11A talks about how to parameterize new contributions to a force-field using first-principles.
11B choosing a "chromophore"-- rigid fragments vs conjugated segments. they broke apart the chromophore in a polymer like p3ht using the dihedral angle--although they validated this choice with other methods. Interesting-- they replace the chromophores with those which have been geometry optimized using DFT. then they make a neighbor list
I'll come back to this paper later. I at least was able to answer some of my questions about choosing a chromophore.
11C talks about calculating the transfer integral for each pair and why ZINDO works and when it breaks down.

rereading the perspective paper \cite{Jankowski2019} looking for things to read next:
Braun 2018 foundations in molecular simulation
https://doi.org/10.33011/ livecoms.1.1.5957
74-89 are relevant to coarse-graining
92-95 start here for opvs

Today I'm thinking about some of the big picture stuff related to my proposal:
Science-wise I am thinking of my proposal like this: Humans use energy and burning fossil fuels to get that energy really sucks. rising CO2 concentration in the atmosphere contributes to global warming. Sustainable, low-carbon footprint energy generation is vital to protect the precious climate which sustains all life. Energy generation using OPV devices is one form of alternative energy. But there are a lot of potential OPV molecules out there! So how can we develop inexpensive, fast methods for screening this ZOO of compounds? Well we can use modelling techniques and a computer. Computational simulations of organic photovoltaics suffer from a problem of length scales: OPV morphologies can have bulk features on the scale of 10s of nanometers (BHJ, exciton dissociation length, pi-stacking) which affect the morphology, yet still we are interested in the electronic properties of this system--a fundementally quantum mechanical property--taking place on the angstrom scale where with current computers we can only realistically look at up to 100's of atoms. <so we break apart the problem MD to get a morphology, Zindo to calculate energy splitting of neighbors-->transfer integral, KMC to estimate CT, talk about results of evan/matty/votca> What am I going to add? more robust/automated coarse-graining and backmapping, calculation of reorganization energy, chromophore detection.
Things I care about: The environment--I like working on a project that theoretically could help find the next molecule for OPV technology. Sometimes it seems very far removed, but having sustainable technology as an ultimate goal matters to me. I also care a lot about help make science more accessible to everyone: I find myself really excited about creating tools which help to visualize the unseeable and to communicate complex ideas in a simple understandable way. I'm not sure this has anything to do with anything, but it's definitely a developing passion.

I'm also reading this paper today https://arxiv.org/pdf/2003.02031.pdf which is a bout TRUE simulations.
notes:
GUI's hurt reproducibility, MS Excel hides the formula's and order of calculation (p 4)
poorly documented code and raw data files dont help reproducibility (looking at you votca) (p4)
*I want to include start to finish notebooks with any paper I publish...* Also docker containers with my exact software stack.
I love the examples in this paper :) It feels a lot like our perspective paper! I'm skimming them but I love that notebooks are included in the SI.
They qualify the GUI's are bad statement saying that GUIs are only bad *if they hide the workflow details.* 

Matt Thompson got a job at OpenFF, one of the tools they list as desirable for applicants is knowledge of psi4:
https://pubs.acs.org/doi/abs/10.1021/acs.jctc.8b00286

read official papers for mosdef. check github.

Reading 

\cite{Hanson2016} 

also my mendeley overwriting my files because it thinks they are duplicates or something? It keeps losing the pdf even though I've uploaded it. :(
SMILES - simplified molecular-input line-entry system
SMARTS - smiles arbitrary target specification
around since 1980s
alternatives SLN, MQL, CSRML.
This paper is about specifically Jmol smiles so I'm skipping to a different one.

\cite{Weininger1988} 

First paper about smiles- how do we unambiguously represent a chemical structure? historically CAS, graph theory, IUPAC.
human-readable and machine readable
hydrogens, single bond don't need to be specified. designed for organic molecules.

\cite{Smith2018} 

reading about psi4: free and open source with python api that uses blas and c under the hood. has jupyter notebooks tutorials. This paper reads like our perspective paper in tone :hearteyes: 2.2 -2.3 kinda lost me. I should look at their jupyter notebooks.

Today searched "coarse grain backmapping" in the library and got the following papers:

\cite{Alessandri2017b}

Coarse graining in OPV bulk heterojunctions! p3ht and pcbm, solvent evaporation. morphologies need large interfacial area between donor and acceptor and relatively high crystallinity and connection to the electrode.How can we investigate the processing conditions that achieve the highest performance?
Analysis of OPVs include AFM, SEM, TEM - provide info about interpenetration (how the two phases are dispersed.) Electron tomography can study 3d morphology., but has limited contrast in organic material due to low scattering. GIXD, GISAXS quantifies the degree of crystallinity but averages over the structure. 
CG allows us to look at these systems but CG models are currently not transferrable meaning that for each new system a new CG model must be developed which is slow. Simulated annealing is often used but a more relevant to experiment method is simulated evaporation.
This paper used CG model like Lee and Pao (read this paper) but also based on finer model like martini. Martini does backmapping. (backwards.py) 
They use explicit solvent (CB/benzene) and they remove molecules randomly as the simulation progresses. Longer drying time results in larger crystallites/ fewer D-A contacts. P3ht shows backbone (pi-pi) stacking at a distance of 4.5 A in the CG morphology. They use backwards.py to do their backmapping. They use 3 beads to represent thiophene in p3ht with no explicitly anisotropic interaction. Carillo (ref 26) used anisotropic addition onto very simplified model (thiophene is single bead) of lee (ref 22). (read these) The model used here thiophene is a triangle and they think this helps it stay planar and stack.
Figure 5 is beautiful.

\cite{Haxton2015a}

CG beads with orientation. They use the word "stiffest" to describe degrees of freedom--they mean bond stretching, angle bending, and torsion within rings. They suggest there are two separate ways to improve CG: better mapping between AA and CG or better CG interactions. The seconds can be helped with IBI, entropy minimization... First one is what the authors will focus on.
Point-site method maps groups of atoms onto a bead--bead potential is spherically symmetric, but some models use neighboring sites to introduce some directionality. e.g., Alemani et al created a protein model where the dipole interaction was a function of the alpha-carbon angle. Here they add orientation to the point-site model. This increases the degrees of freedom for each site from 3 to 6, but they model bonds and angles as stiff and dihedral angles (except rings and double bonds) as soft. They also map about 8 atomns to a site. They use backmapping from ref 64--not backwards.py. 
They simplify the ideal interactions of the CG model (many-body problem) to pai potentials. They show how to calculate to error in an observable calculated on backmapped structure. C code used for backmapping in SI.
mapping ratio - atomic DOF / CG DOF. PRIMO on average is 3.0, UNRES, MARTINI, MS-CG are 8.2, 7.1, and 6.0. I think this paper aims to improve on PRIMO by adding directionality? they measure accuracy with rmsd and efficiency with mapping ratio.

\cite{Wassenar2014b}

This paper is about backwards.py. The authors are from university of groeningen where GROMACS is made. however this tool is generic and not specific to gromacs. it uses geometric projection and position restrained MD relaxation--same as what Matty did. can map between anything just needs a mapping.
Good overview of CG methods:
Rzepiela - randomly place atomistic beads on small spheres around CG beads. Requires many steps, specific to one verison of GROMACS, but can be made specific with correspondence between atoms and beads.
Brocos - lipid micelles. placing atoms on a trace through the CG bead and adding peripheral atoms at offset. 
A good CG model should yield correct structures and be simple quick and flexible. Should be generic (ff and system type) and require little information. Then information provided should be the CG coordinates, the atomistic topology and the correspondence mapping between the atomistic and CG particles.
They go from MARTINI-CG to GROMOS 54a7 UA to CHARMM36 AA.
Some matrix mapping that I don't totally understand. Confusing between A the matrix and A the atom position. :/
Fig 5 is neat! c: shows a helical protein at each step of the CG->AA process. their procedure appears to improve upon the simulated anealing of rzepiela. 
* look at backwards.py with the intent of better understanding the Matrix transformation.

\cite{Peng2019a}

biomolecules. Backmapping based on Bayesian inference and restrained MD. cgmap.py -- they tell us the names of their codes but don't provide them?? They try multiple CG methods: Martini (twice with different initial structure), UNRES, ENM. Martini had the lowest RMSD but it is also the finest grain method. They have an ultra coarse method that only use 5 beads for a small protein; it gets impressive results (RMSD 0.47 angstrom) for how sparse it is! But it is not sufficiently accurate to predict structure-- other RMSD > 10angstrom. their method is more expensive. I don't quite understand how they get some of the probabilities for the Bayesian part, but my takeaway from this paper is that the logrestraints work a lot better for them and Bayesian inference, although expensive, allows them to skip the AA MD simulation at the end?

\cite{Rolland2020a}

This is the paper we reviewed in lab meeting! CG -> backmap -> mobility. AA MD can reproduce XRD patterns pi-pi stacking, formation of lamellar structure, change in morphology at high oxidation levels, mobility dependence on chain-length (* looks into ref 26 and compare to evan's tie chain paper)
CG allows us to look at larger lengthscales and with backmapping we can restore atomistic details, but is there a cost? ref 44 model HOTT molecules as ellipsoids with Gay Berne interactions (I'm thinking of this as anisotropic--i.e., directionally dependent so good for ellipsoids--Lennard Jones. Often used in liquid crystal systems.) 
They use the same backmapping as us ref 13. Many CG papers verify by XRD buyt they claim same XRD can result in very different mobility. ref 26. They use Wassenar's backward.py script. -- He is from the university which makes GROMACS
CG overestimates density--backmapping fixes it. XRD of backmapped AA match AA. pipi stacking is corrected and lamellar structure present. Mobility also good. They explain the differences between the AA and CG mobilities saying that the AA one is smaller so it's a size effect due to over estimated percolation threshhold--i.e. the connectivity is overestimated because the periodic volume is small enough that the periodic image has an effect.

\cite{Poblete2018a}

RNA - backmapping steered MDaimed at minimizing a particular RMSD (ERMSD) between atomic and target structure. target structure were found to be recovered by this backmapping except in the case where there were unpaired bases. folding pathway dependent on parameters used in ERMSD.
need AA strutures to validate with experiment (x-ray or neutron scattering) backmapping usually involves placing atomistic coordinates relative to the beads and then minimizing the energy of the structure using steepest descent or short MD runs (could use position restraint). Atomistic can even be mapped onto the whole trajectory. 
This work describes the backmapping of the SPlit and conQueR (SPQR) method. Minimizes the ERMSD distance between solvated AA strand of the CG structure (RNA). ERMSD : origin is set to CoM of C2 C4 and C6 atoms of each base (see ref 23), then for each pair of bases, rij is the center of base i in the coord system of base j. In order to account for anisotropy, the scaling for the z-direction is different (does this mean the strand is oriented to z in some way??) This metric shows the difference between arrangement of nucleobases of two RNA structures. Backbone and sugar interactions are unrestrained. The paper will show results on the backbone conformation. They use steered MD with ERMSD as the steering parameter. (How does steered MD work?) Look like they use and external pulling force which depend on ERMSD.

Of these the Alessandri paper was the best and  most interesting/relevant. I am pulling some refs from here for next time:

\cite{Mikhnenko2012}

Exciton diffusion length (EDL) in narrow bangap polymers. photoluminescense decay time is related and measurable. PCBM and PCDTBTs modeled with Monte Carlo--code available. They used P3HT as reference (5.4+/-0.7nm exciton diffusion length). 
Exciton diffusion is essential for organic solar cells. excitons are electron hole pairs created by excitation due to light absorption. they are separated into free charges at an interface which creates current. the movement of the excitons through hopping is what is refered to as diffusion. Exciton diffusion length is a factor in device efficiency.
Other methods to measure EDL are fluorescence quenching - the dependence of the exciton quenching efficiency vs semiconductor thickness is fit to 1D diffusion equation. Pros: direct measuremnt, cons: difficult experimentally and there are many things to consider in the model (FRET, effect of polymer-vaccuum interface, optical interference). 
This paper uses MC to model PL decay.
(Losing focus come back tomorrow)


Ugh. I think I've finally fixed my Mendeley library?

\cite{Scharber2013}

Review of state-of-the-art OPV BHJ solar cells. 
pros: thin/light, semitransparent, cheap, continuous production process, short payback times, easy to integrate into new and existing products. 
History of development from 2001 2.5\% PCE to 2011 10\% PCE. Most degrade when exposed to air and sunlight within a few years. Molecular structure including side chains affects stability. Electron affinity of the acceptor determines degradation rate of the semiconductor. Changing the elctrode and using the inverted design(? read waldauf) helps the electrode oxidation. This requires no vacuum
They discuss BHJ morphology and how the A and D regions need to be small enough that an exciton could find it's way to an interface but not so small that once the charge is separated and current is generated at that interface that the current couldn't get to the electrode. The ideal is an interlocked comb structure but in reality solution processed OPVs form patches. This review is not about morphology (they recommened Hoppe2004). Morphology can only effect the efficiency of an AD pair, not the ultimately efficiency limit. This depends on the bandgap and electron affinities of the DA moeities.
fig 5 explains inverted design a bit. 
They discuss costs and efficiencies. Should probably read about Shockley Queisser Limit. (ref 47) based on analysis of pn solar cell they say the max efficiency is 30\% and the optimum band gap in 1.1 eV. Archer and Bolton roughly confirm this -- much of the light energy is lost because the photon is too small to excite the absorber, other loss is due to intraband thermalization of charge carriers (?), 12\% entropic loss, 1\% radiative recombination. Assumption of the Shockley Queisser limit
1. each absorbed photon generates and e/hole pair
2. perfect charge collection
3. radiative recombination as the only recombination.
Based on this work and the external radiative efficiency (ERE) proposed by Green, and actual max of 15-16\% efficiency is predicted.
(continue from 5)

\cite{Waldauf2006}

Read this with intention of understanding inverted design

\cite{Hoppe2004}

Overview of organic solar cells. sp2 hybridization of carbon is what allows organics to absorb in the uv-vis range. (conjugated pi orbitals.) In comparison, inorganic semiconductors have low mobilities but very high absorption coefficients. The exciton diffusion length is shorter in OPVs too.
Most OPV are hole conductors and have band gap around 2 eV, which is higher than silicon. But the ability to modify and tune the chemistry as well as the cheap, mass production makes OPVs enticing.
history: 
first OPVs were single organics layers sandwiched between two metals with different work functions. (Evac - Efermi) 0.7\% PCE
next bilayer heterojuctions-- two organic layers between electrodes (e.g. perylene and phtalocyanine) 1\% PCE
C60 introduced, polymer fullerene bilayer heterojunction.
Bulk heterojunction 2\% PCE
(This article only discusses up to 4\% PCE and does not provide specific explanation for the increase)
common structure is hole-conducting polymer with electron acceptor (C60 as in PCBM). Solution processable due to side chains. Preferential orientation of polymer backbones parallel to the substrate gives rise to anisotropic CT. In BHJ cells CT is sensitive to the morphology.
OPV energy generation steps: 
1. absorption
2. exciton diffusion
3. charge separation
4. charge transport
PCE depends on # photons absorbed, dissociated, and that reach the electrode. Number absorbed depends on the absorption spectrum, absorption coefficient and layer thickness. The fraction of excitons which dissociate depends on the exciton dissociation length and the charge separation probability at the interface. Finally the the number of charges which reach the electrode depends on the driving force or gradient resulting from the potentials of the elctrons and holes. thin films (<100nm) are usually field drift dominated others have enough screening are dominated by concentration gradients at the contacts.
I skipped over devices.
concepts for improvement:
metal electrodes quench electrons in C60 so an transparent, exciton-blocking layer is between electrode and layer with C60. ultrathin LiF interfacial layer is one example. 
Increase the absorption range. smaller band gap <2eV ~800-900 nm.
morphology controls. to control size of fullerene domains in p3ht/fullerene, a plasticizer was added. This increase miscibility of polymer and fullerene creating smaller domains. Solvent/evaporation time, surface interaction, and annealing all have a large effect of the morphology.

\cite{Scharber2006a}
Design rules for donor in BHJ solar cells
interpenetrating network of C60 and conjugated polymer have shown ultrafast charge transfer (~40 fs). There is no competing decay process in this timeframe. optimized mixture with C60 converts absorbed photons to electrons with efficiency close to 100\%. At time of publishing best PCE is ~5\%. Efficiency is limited by low open circuit voltage of device under illumination. Organic semiconductors with bandgap of 2eV are used but observed $V_{oc}$ is in the range of 0.5-1 V. $V_{oc}$ in a metal-insulator-metal is equal to the work-function difference of the two electrodes. In polymer-fullerene $V_{oc}$ is affected by the morphology of active layer, electrochemical potential of the cathode. Ref 15 (Gadisa) studied correlation between electrochemistry andVoc of  polymer fullerene with PCBM and polythiophene and found that variation of the oxidation potential of the polymer caused variation in the photovoltage. (This is not the same as $V_{oc}$ ?) This paper suggests that Voc is related to HOMO of the polymer. and thus the maximum efficiency can be predicted.
I am confused by a lot in the paper and don't understand why Voc is the limiting factor. Fig 2 is interesting, shows how they predict PCE from HOMO/LUMO and bandgap. Should Eg (bandgap) be LUMO acceptor - HOMO donor?
OK so they don't really give suggestions for what kind of chemistry makes a better donor, but they do say that measuring the HOMO of the donor can tell a lot about the PCE of the device.

\cite{Yu1995}

Older paper about BHJs. (is this maybe the first about ADA BHJ?) MEH-PPV efficient charge transfer result from bicontinuous network
time scale for CT from donor to acceptor is <picosecond, $10^{3}$ x faster than radiative/nonradiative decay, so efficiency is near 100\%. Just adding 1\% C60 increase photoconductivity by order of magnitude.
conversion efficiency in a bilayer heterojunction device is limited when photoexcited exciton can't get to the interface. So an interpenetrating network is ideal because it maximizes interfacial area and minimizes the distance and exciton must travel to reach an interface.and if the network is bicontinuous then transfer of the charge to the electrode should be just as efficient.
ITO (indium tin oxide) I thought was to prevent the active layer being quenched by the metal but they're saying it's the anode? PCBM is a soluble C60 derivitive. Ca electrode used.
Pure MEH-PPV Voc 1.6V is consistent with the difference in work function of Ca and ITO. In the MEH-PPV/PCBM mix the turn on current shifts lower by 0.8V thus LUMO of PCBM is 0.8eV below Efermi of Ca. 
They predict greater efficiencies if the morphology is optimized.

\cite{Halls1995}

another older paper. Cavendish lab! Photovoltaic effect - production of electrons/holes under illumination. Molecular semiconductors produce excitons instead of free charges. Charge collection requires dissociation, which is efficient at interfaces of different electron affinities and ionization potentials. (electron goes to larger electron affinity and hole to lower ionization potential.) A layered structure then seems ideal, but Exciton diffusion length is much shorter than the absorption depth. so an interpenetrating network is shwon in this paper to be ideal.
poly-phenylenevinylene derivitive mixture MEH-PPV and CN-PPV. spin coating on glass slide. they claim phase separation on the scale of 10-100nm this measurement relied on ferric chloride doping MEH-PPV but not the CN derivitive.
The current vs Voltage graph for the mixture was very different under illumination.
they predict these sort of flexible polymer photodetectors will be useful in medical imaging.

\cite{Gartner2019a}

Somehow didn't add this!
University of delaware. There is a Jankowski/Marsh image in Fig 1!
OMG I just realised this is Arthi from the MoSDeF calls. :D No wonder this paper is so approachable!! 
Atomistic structures, not always more accurate, can't compute polymer chain level rearrangement
CG -- chemists might not like them but physicists do
atomistic simulations are good when you already know the chain configuration at a state point. Atomistic FF are still tailored to fit the properties of specific systems. got distracted today... will continue with II.B tomorrow
OK back to it
CG - intermediate resolution (group of atoms within a monomer), CG monomer (whole monomer), mesoscale CG model (group of monomers on the length of a Kuhn segment or the whole chain.
Finer models are parameterized to represent specific chemistries while coarse models are more generic. finer models can be parameterized with IBI for structure or force matching or relative entropy matching. the level of coarseness should depend on what features you want to examine.
MARTINI - generic model 3-5 heavy atoms are grouped together each bead is assigned polar/apolar/donor/acceptor etc based on the chemistry/connectivity.
soft potentials can be used for CG beads like DPD -- can have unphysically high compressibility
!!! Systematic coarse graining approaches refs (87,91,269-273)
start reading tomorrow at pt III

simulation method (MC or MD) three steps
1. create/initialize system
2. equilibrate
3. sample
all steps should avoid bias.
also choose ensemble, thermostat, barostat... choose for what you want to know. e.g. phase behavior - GCMC or GEMC phase coexistence with bead insertion/deletion. don't have to model the interface. Ref 308 is a GEMD method devolped by Arthi's group!

Size: big enough that you can observe the phenomena you looking for. Calculating Flory-Huggins or polymer-polymer second virial coeff require long length scales. phase separations must also be larger than the length scale of the concentration fluctuations. However if interested indilute-soln properties like chain configuration or scaling laws--single chain even can be appropriate.
finite size effects emerge from self interaction with the periodic image and limited size of box/system. smaller size can lead to pressure and stress fluctuation preventing calculation of mechanical properties.
good rule of thumb -- box length to radius of gyration should be L/Rg >= 5.

initialization: random initialization might be fine if system is isotropic but not at high density. A procedure to initialize high density CG polymer melts which uses MC with no intrachain moves and soft MD is described.
an important metric for chain configuration is mean squared internal distance (measure of atoms or beads separated by n bonds along the chain). shows perturbations in internal chain not shown by the radius of gyration or end to end distance.
We use a shrink-step, but that isn't mentioned here...

Equilibration:
relaxation in chain configuration is slow compared to relaxation of chain level metrics (like the mean squared interanl distance) or check MSD of CoM of polymer chains -- if they diffuse nultiples of Rg over equilibration, the system is relaxed. annealing processes (change T and P (also V???)) can help equillibrate. also mimic experiment.
Start again at p.11 Sampling
decorrelation is expecially important for polymers as the relax very slowly. Calculate the decorrelation time of a property of interest like Rg (for polymers this might be better than the decorrelation of the potential energy!)
for systems with high energy barriers to rearrangement, small independent simulations may be better than independent snapshots from one simulation.
Advanced sampling techniques: umbrella, Wang-Landau, adaptive force biasing, etc. used to explore free energy landscapes with mulitple barrier between minima more efficiently.
Analysis - focused on structural and thermodynamic properties -- not dynamic properties.
Looking at simulations is great, but it must be paired with quantitative analysis. Always report error bars. good way to do this is block averaging: simulation (trajectory?) is split into blocks the mean value of a quantity of interest is computed in each block, the average and std dev of the mean from each block which then become the ensemble average and std err for the entire simulation.
RDF and structure factor characterize system of many length scales as well as allow access to thermodynamic quantities of interest. Sq can be directly compared with experiment and analysis of low q (long length scale) provides insight about phase separation and concentration/density fluctuations. However--range of q that can be accessed in simulations is limited to longest real-space distance that can be reliably sampled in box. Two methods to calculate Sq (fourier transform of g(r) and summation method) the fourier transform can introduce unphysical artifacts because it is only rigorously defined over infinite spatial domain (impossible for a simulation box.)
other scattering techniques include reflectivity, small-angle/wide-angle scattering, diffraction (jankowski marsh paper cited!). They even mention dynamic S(q,t) can be used to understand chain relaxation.
Flory Huggin parameter - lattice model of thermodynamics of polymer solutions takes into account different molecular sizes when calculating entropy of mixing. (this paper goes into depth about a specific thing about this parameter but as I have only just heard of it -- the summary above is from wikipedia-- I don't understand what it is talking about yet)
Phase Transitions - different order parameters (Rg, end-to-end distance, etc) may be necessary to find a phase transition. Simulations that use a temp/pressure ramp may show hysteresis. 
Using analysis software - make sure to validate using previously published results.
Made it to section IV! finish paper tomorrow. :)
Comparison with experiments - CG model can have quantitative agreement with universal parameters based on polymer physics like polymer chain size scaling with MW. Otherwise it is unreasonable to expect quantitative agreement. Although they may have agreement for more physical parameters, same goes for atomistic systems. Always need to recognise the limits of your model and have good expectations. Example: Tg in glassy polymers often found by the intersection of linear fits of density vs temperature above and below transition temp. To obtain density vs T the system is gradually cooled but even the slowest cooling rate is ~10 orders of magnitude faster than experiment. This is known to raise Tg significantly. Tg needs correction. but it is useful to examine trends in Tg--polymer architecture, chemistry, MW. and expect similar trends in experiment. 
Because of free energy landscape flattening in CG, dynamics can only be qualitatively compared. Also keep in mind when comparing to an instrumental analysis, the limits of the instrument.
Comparison to theory - limitations include finite size effects, inability to capture phenomena without advanced sampling, inability to capture time scales (fast molecular vibrations vs polymer relaxation). Combined simulation with theory where simulation feeds parameters into the theory model.
DFT-with MD/MC can be used to calculate the free energies in structure during solvent evaporation.
PRISM - don't quite understand what this is something like a way of understanding the structure of a polymer morphology?
Finished! OK paper takeaways -- autocorrelation for polymers shouldn't be based on PE, need to learn how to calculated some of the polymer metrics.

\cite{Muller-Plathe2002}

linking length and time scales. Analogy plant cell metabolism is very fast but tree death due to pollution is very slow; it is unecessary to measure tree dieback every milisecond. this analogy in chemistry ca go from observing bonding with electron wavefunctions, to modelling solvation with explicit water and FF, to modelling the flow of bulk water with a continuum characterized with various parameters. coarse grain models are built rigorously through converting the results of a fine grain system to the parameters of the coarser system. This efficiency gain allows us to indirectly predict large-scale and long-time quantities. 
Backmapping allows the coarse method to generate *samples* for the fine-grain method.
mapping atom to coarse grain bead reduces the DOF and the effective potential account for the missing DOF. CG potentials are usually softer which allows larger time-steps. CG models are specific, tuned to a specific chemistry and to reproduce a specific structural or thermodynamic feature. 
A penalty function can be used to determine how far off the CG model is.
The degree of coarse-graining and where the beads are placed can affect the success of reproducing the structural/thermo feature. They show an example of a diphenyl carbonate represented by 3,2,1 beads. The 3 bead structure easily replicates the atomistic RDF with just LJ6-12, the 2 and 1 bead structures do not, but the 1 bead structure can be made to fit the atomistic rdf if a piecewise potential is used (overfitting?)
Cg optimization procedures: two procedure are describe here the first optimizes Gaussian functions--I don't understand what or how they are optimizing. the other uses elliptical potentials to give space filling at experimental density.
start next time at section 3.3
Using a DOF distribution (pairwise, bond stretch, angle) to get a CG potential. An analitical function. the potential obtained this way really is a free energy not a potential energy. It incorporates effects from finite temperature and neglected degrees of freedom.
IBI is a way to get a numerical potential (For IBI Reith is cited -- he appears to be Muller-Plathe's PhD student.) The description of IBI is really concise and clear. :) IBI is a tabulated potential. They describe two other methods.
Potentials can also be optimized to match thermodynamic targets. They discuss how to add a cutoff value to IBI potential. also cite a paper that adds pressure information to numerical potentials.
skipping lattice models.
Atomistic -> derived CG potential -> CG sim -> backmap to atomistic -> local relax. 
Seems roundabout but it's not! Smoothing of the PE surface in CG allows easier access to polymer morphology. CG can provide starting structures to atomistic MD. which can then be inputs to electronic structure calculations. 
CG is so much more efficient we can calculate chain properties. AUTOMIZATION IS KEY (IBI, simplex). The intellectually challenging part of CGing is where to place the beads!!
errors in atomistic FF will filter to CG ff.
CG models are specific to the conditions under which the parameterization is performed.

\cite{Reith2003a}

IBI algorithm is made in this paper! Uses potentials of mean force. IBI potentials are nontransferrable--they work best for the system they are optimized for.
CG allows chain behavior to be studied. History: bead spring polymer chains in a melt to a soft-core liquid ellipsoidal particles with anisotropic gaussian potential. polymer melt cg-ing. None of these derived an automated optimization scheme to conserve the chemical nature of the underlying atomistic model in a standardized way as we do.
PRISM theory combined with MC can predict long and short range structures of polymer chains but can't produce CG potentials.
Reverce MC can reproduce RDF without a need for potential. lack of potential limits usefulness though because RMC uses knowledge of RDF without knowing underlying forces.
Description of IBI algorithm: potential of mean force IS boltzmann inversion of RDF. (Free energy NOT potential energy.) Bending potentials before nonbonded potentials.
They used a five point running average to smooth the potentials
bond stretch -> angle bend -> nonbonded -> dihedral
They started with monoatomic liquids with LJ and WCA potentials (1) they used the correct potential as V0. (2) then they used the boltzmann inverse of the RDF. 
Conclusions:
start with small range (cutoff?) and increase until RDF matches. CG FF for polyisoprene are different between solution and melt.

\cite{Ayton2007a}

Coarse graining scheme, systematic, for biological systems.
If CGing is done systematicall it retains important averaged atomistic level interations. Connecting length and time scales.Systematic methods for making CG systems: RMC, this paper will focus on multiscale coarse graining (MS-CG) uses force matching algorithm
Variational. Some of the math does not make sense but it appears to only be based on positions of atomistic and CG particles. They make the free energies of the atomisitic and CG systems the same. Many atomistic frames are mapped to CG beads and the potentials are based in the gradient of the forces felt by the beads. The algorithm minimizes a residual function.
Summary: works, RDFs match. Enables calculation of CG force fields without resolving many-dimensional potential of mean force. (but honestly IBI seems so much simpler. Is there something I'm missing? is boltzmann inverse of RDF not PMF?? This paper was published after Reith paper... when is force matching better?)


\cite{Peter2009d}

Review paper- coarse grained models linked to atomistic models. 
They mention the good, marginal, and poor solvent parameter Theta.
models can be linked sequentially, hybrid - both level present at once and interacting, or adaptive - individual molecules switch on the fly depending on e.g., spatial coordinates.
CG target can be thermodynamic or structural.
Structure based: boltzmann inversion of distribution with constant to set minimum to 0. Free energy vs PMF? Boltzmann inversion usually results in a tabulated potential but by fitting to gaussians it can be mapped to an analytical function.
Bonded potential can be determined on a single polymer chain in vacuo but then the nonbonded interactions have to be considered carefully to avoid double counting. However bonded and nonbonded can be obtained from the same atomistic simulation of polymer melt iteratively. This makes the potentials interdepent.
IBI can also consider other thermodynamic quantities like pressure and compressibility. IBI not useful for dilute solutions
start next time at backmapping
backmapping has no one unique soln. every CG structure corresponds to an ensemble of atomistic microstates. reinsert theatomistic structure so that the atoms still satisfy the CG mapping and then relax the atoms with position restraint to the CG position.
CG backmapped to atomistic allows better comparison with experiment than atomistic alone by giving access to larger length and time scales. e.g., NMR timescales are too long to simulate with atomistic alone, yet atomistic coordinates are necessary.
Dynamics: the CG model is independent and has it's own timestep. dynamics are accelerated.


\cite{Baschnagel2000}

Bridging the gap between coarse and atomistic. Specific to polymers. This paper focuses on the mapping of atomistic to CG. (cool! first paper to focus there.) also discussed backmapping. Just noticed this paper is 150 PAGES LONG! OK time to skim.
Polymers have properties which can be measured at many different length scales. diffusions constant can be measured by monitoring the center of gravity of the chains.
It is too big and too slow and also just a bad idea to simulate huge boxes of atomistic polymers. Even if the simulation was to complete, there would be too much information to process accurately. CGing allows scientists to FOCUS their experiments.
start next time section 2 (p8 of 116)
Frozen nuclei approximation is incorrectly referred to as the Born-Oppenheimer approximation?
Quantum chemistry is good for predicting bond and angle potentials but not for LJ because you would need huge basis sets and high order electron correlation. DFT has similar issues LDA results in overbinding and GCA no attraction. partial charges are conformation dependent. 
Quantum chem is very useful for parameterizing dihedrals!
I skimmed this section v heavily but my takeaway is that if you need to know about a chemical reaction (transfer of electrons) it makes sense to consider QC methods. Also QC methods are not the cure-all for making forcefields.
section 3 - how do we get dense systems?
it is hard to change the system once it gets to a dense atomistic representation. so one should start with a good systm! (like from coarse-grain)
This is tough because polymers are dense, amorphous, and highly-connected. three methods exist:
1. add new segments according to the metropolis criterion
2. coarse initials guesses created on a lattice
3. low density condensed step by step using NPT
they described a method of initializing using stream lines and vector fields which results in polymers of constant curvature.
They describe a new rotational move that does a dihedral cis-trans flip.
next time skim through the rest of this paper. I'm over it
read mazzio paper!
gave up on baschnagel paper--switching to mazzio!

\cite{Mazzio2015}

oh this paper is so much better. 
opvs are lightwieght, low cost, flexible. Metrics for evaluating these are PCE, Levelized cost of energy (LCOE) and energy payback time (EPBT).
LCOE is life cycle cost/energy produced. EPBT is how long it takes for a device to pay for itself.
start next time at section 2
inorganic: absorptions of photons with energies greater than the band gap results in free charge carriers that can separate at an p-n junction. they can diffuse under external electric field to electrodes.
organic: lower dielectric constant which prevents screening of the attraction between electrons and holes this results in exciton formation rather than free charges. separation of the exciton requires an acceptor/donor interface. if the exciton doesn't reach an interface it will decay back to ground state resulting in the loss of that energy. donor: large ionization potential acceptor: high electron affinity.
different loss mechanisms are discussed: geminate recombination is where the separated charges recombine across the interface. charge recombination where free charges recombine with unassociates charge carriers in a device.
There is some about electrochem which I am reading but not understanding :( external quantum efficiency is a measure of how much current will be produced by a particular wavelength of light. it depends on the efficiency of photoabsorption, exciton diffusion, charge transfer, charge collection.
CHARGE TRANSFER is not the same as CHARGE TRANSPORT
ASTM reference spectra are used now to characterize and standardize OPV
3. Device architectures
some history about opvs: Tang 1979 introduced the two-component donor acceptor active layer with thermally evaporated small molecules. bilayer structure. limited by exciton diffusion length. 
BHJ with fullerene acceptor reported by Yu in 1995 -still state of the art morphology.
diagrams of different device architectures.
start next time on section 4
materials design (aka the good part :D )
good opv acceptor/donors are solution processable, have appropriate band gaps and energy levels, and long planar pi-stacking for good charge carrier mobilities.
regioregularity (head-tail vs head-head p3ht). Mn is the number average molecular weight of the polymers. weight of all polymers/number of chains. Mw is the molecular weight per unit weight of polymer (accounts for size distribution). dispersity is Mw/Mn.
commonly used materials: initially poly(p-phenylene vinylene) (PPV) was very poplar. It was mixed with PCBM (c60) in first reported BHJ solar cells. now we have shifted away towards poly alkyl thiophenes like p3ht.
p3ht/pcbm was max EQE of 76\%. post processing like thermal annealing with externall electric field produced devices with max PCE of 3.5\%. it has been shown the p3ht and pcbm have strong dependence on regioregularity. longer chains are shown to perform better, due to increased mobility with polymer aggregation.
ways to decrease the band gap have become popular using DA polymers (alternating electron rich and electron poor heterocycles) PCDTBT achieved efficiency of 7.5\%. PTB7 had PCE of 7\% and then over 9\% due to promotion of the quinoidal resonance structure using thienothiophene. quinoidal stabilization improves shortcircuit current (Jsc) and fill factor (FF) which increase EQE and IQE so it has good exciton dissociation, charge transport and extraction.
Some small molecule (not polymer) devices show promise.
fullerenes are commonly used as acceptors. the added groups are to help solubility. pc61bm has limited absorption and deep lumo so it can be improved by using pc71bm which is less symmetric and has more allowed optical transitions. indeneC60 is being used to adjust the LUMO level.
next time section5
morphology
the ideal is an interpenetrating network of d-a materials with domain sizes on the order of the exciton diffusion length (10 nm). Enough interfacial area to dissociate lots of excitons while maintaining continuous charge transport pathways. Parameters that affect the active layer are solvent donor/acceptor concentrations, thermal annealing times/temps, solvent annealing, additives, and interlayer surface energies. 
solvent - vapor pressure, boiling point, viscosity, polarity, solubility, wettability dictate much of the initial film forming. solute properties like concentration, blend ratio, solubility also important. and finally the method of deposition matters-- high-throughput processing will require alternatives to spin coating but spin-coating is most widely used so it's what this review covers.
switching the solvent in a PPV/PCBM from toluene to chlorobenzene improved performance by nearly three fold! the solvent allowed more mixed/smaller domain sizes. fullerene has better solubility in chlorobenzene.
same active layer compounds in chlorobenzene or toluene, switching the total solute concentration and the ratios had an effect on the morphology but chlorobenzene prevented fullerene aggregation until very high loading.
a study also showed that slow evaporation allowed for separation of p3ht/pcbm but fat allowed for formation of an amorphous glass.
Thermal and solvent annealing allows for larger domains in p3ht/pcbm. annealing increase the hole mobility --> this is the most important factor in increased performance!
Chen (ref 41) studied p3ht/pcbm with GIXD. found that annealing results in bicontinuous network of polymer/fullerene with domains on the order of the exciton diffusion length.
Solvent annealing where layer is annealed under vapor. shown that poor solvents better enchance performance then good solvents. good solvents develop greater self-organization of p3ht -- but the domain were too big. solvent annealing promoted PCBM separatiun to the top of the device.
interpentrating network has been the ideal, but crystalline regions of acceptor donor with mixed amorphous regions is more realistic.

\cite{Brini2013}

review of systematic coarse-graining for soft materials
focuses on representability (how well a system predicts properties at the statepoint it was optimized for) and transferrability ( how well it can be used for other statepoints.)
effective pair potentials are used for CG instead of the expensive many-body potentials. averaging over fast degrees of freddom removes their entropic contribution. to compensate for this, the pair potentials are softer resulting in a smoothed energy landscape. this affects the thermodynamic properties-- e.g., the heat capacity, which is related to the entropy fluctions at constant t and p is underestimated by a cg model (because the fluctuations arent as large). while compressibility related to volume fluctuation will be over estimated (i think because the potentials are so soft.) The thermal expansion coefficient which is related to both may well be represented accurately.
parameterized vs derived CG potentials - 1st one atomistic simulations are used to calculate target properties (pair correlations, force distributions) IBI is this. 2nd one atomistic interaction are used directly. not optimized to reproduce selected tragets, therefore all properties are predictions of the model.
Parameterized methods: IBI method described. 
Inverse Monte Carlo - also an iteractive scheme that corrects a guessed interaction potential in order to reproduce a target radial distribution function, but the guess is based on statistical mechanics that take into account that a variation of the potential at a given distance r can lead to a variation of the RDF at all other distances. more expensive than IBI requires longer sims to calculate correlations. Can also include target thermo properties like surface tension.
Both IBI and IMC are cases of relative entropy -- a measure of the amount of information lost upon coarse-graining and is defined in term of propability distributions of atomistic and CG systems. both methods minimize the discrepancy between the atomistic and CG distributions.
Another method is force matching. effective pair potentials match the mean force on a particle through the atomistic force field.
Derived methods: These potentials have a clear physical meaning (why?) potential of mean force calculated at LOW concentration. fragment based coarse graining
skipped ahead to polymers
polymers are important for cg because the volumes needed to observe their long length scale properties in atomistic md are prohibitively expensive. pressure-corrected IBI scheme for polystyrene matched gyration radius and Flory characteristic ratio at 500 K. (flory ratio relates to chain stiffness?) but entanglement length was overestimated.

\cite{Murtola2009a}

quick letter - to get the full picture we have to consider multiple length scales. abstracting small length scales is necessary for time/resources. probably dont include this in references, it's so short.

\cite{Berendsen2010}

ooh this one looks cool. starts with history of membrane models, goes to cging, then moves to cg dynamics. for next time
many realistic process could not--and would never--be solvable by atomistic simulations. he describes a very simplified model of transport of a single water molecule through a membrane. reduce the system to motion in one reaction coordinate.
the heart of coarse-graining is the reduction of degrees of freedom. then describing the dynamics of the reduced system so that it reproduces the projection of the complete system. this is only possible when the reduced and the removed degrees of freedom are well-separated (the removed are much faster). If the time scales are not separate the dynamic behavior will not be accurate but the probability distributions and thus the thermodynamic properties may still be.
The choice of relevant dof is made on an intuitive basis and depends on what properties are wished to study. Note that atomistic md is already concerned reduced--no electronic dof.
if a CG model does not predict any properties beyond the ones you have used for parameterization--you have achieved nothing! always require your model to predict something unknown.
the motions in reduced space: the equations of motion are no longer hamiltonian--forces depend not only on the present but also the past configuration. Forces proportional to the velocity are non-conservative (kinetic energy is lost). in order to maintain average KE and T noise must be added.
I skimmed the sections on dynamic equations and modelling fluids.
simple methods that are easy to understand and implement--provided they are correct and work--will always prevail over complex methods, even if the latter are more accurate.

\cite{Kim2006}

experiemental paper about effect of p3ht regioregularity in p3ht/fullerene blends on device performance--PCE.
really useful schematic of GIXS experiment. (GIXRD)
interplane pi-pi stacking is parallel to the substrate--enhanced interplane interactions with high RR p3ht benefit vertical (J-V) more than lateral (field-effect) transport. TOF hole mobility in p3ht films show neglible influence of RR. inteplane stacking does not affect vertical transport in pristine crystal. in the blend dark-current densities and TOF mobilities show stronger dependence on regioregularity. highest device efficiency achieved with RR p3ht

\cite{Lee2011}

multiscale simulation of p3ht and pcbm. 1 bead per p3ht and pcbm. They used IBI to get their force field.
they use a penalty function to minimize their lj parameters-- I think this is different from IBI in that it doesn't use a table potential.
their reverse mapping method could use a little clarification but it sounds like they replace CG bead (p3ht or pcbm monomer) with equilibrated atomisitc structure and then do only rotation moves until the whole system is relaxed. (thats kind of cool actually!) after MC they do quick MD. smoothing of the energy barriers i the CG sim accelerates the dynamics. CG morphology is in good agreement with TEM and electron tomography. with backmapping even pi-pi stacking was observed.
they use a spatial discretization scheme to simplify the volume into voxels and quantify the interfacial area to volume ratio. as time goes on in the simulation the p3ht domain sizes grow and the interface to volume ratio decreases--this is a bad thing? is this related to the quick annealing having better results??

\cite{Huang2010}

CG polymer fullerene simulation of opvs.
parameterized at two state points.
it is still not know how to predict phase behavior based on components and processing. the molecular scale structure often has to be inferred. the relationship between phase behanvior and charge transport has been investigated but there is some disagreement.
this is where simulation comes in. systems of only a few nm can be studied--but the exciton diffusion length is ~5-10 nm --> CG
ff for super atoms are parameterized using atomistic ff. OPLS-AA 3-site p3ht and one site fullerene. They use IBI to parameterize their ff. they use a a scaling function to match the pressure. (i don't understand how this works exactly)
they found that their model could be useful outside of the temps it was parameterized at. 
they calculated the time scale of the cg model.
they were able to compute 768 p3ht 48mers and (110592 beads) with 4608 pcbm beads in a 25 nm^3 box in about 24hr.

\cite{Jankowski2013}

opv performance strongly depends on morphology. ideal is to have high interfacial area while maintaining high connectivity. 
choosing a model: sufficient chemical and physcial detail without too much to prevent sufficient sampling at relevant length and time scales. if the model is too coarse we miss intecalation of the fullerene between the side chains. 
order-disorder trainsition for neat polymers depend on side chain orientation (regioregularity?) and orientation along th polymer backbone. 
immiscible acceptor in blends do not change the morphology compared to neat but increase the ODT.
CG model gives backbone persistence lengths that match experiment. (has to do with bending stiffness of polymer)
how were LJ params obtained for CG model?
how to quantify interfacial area--acceptor-backbone correlation rdf first peak, 
intercalated layer are seen as the acceptor miscibility is increased-- mostly with the most sparse polymer (do the acceptors feel the most attraction to the backbone?
new lattices predicted--have these been observed?

\cite{Marsh2014}
coarse grain simulations of thiophene polymers (3 site model). changing the spacing and regioregularity of the side chains results in different morphologies and can be related to different real compounds.
orientation of side chains along the backbone is more significant than side chain density, side chain-side chain interactions, or side chain length. side chains on boths sides form lamellae--on one side form hexagonally packed cylinder which can transition at low temp to lamellae or ribbons depending on side chain interaction strength. interaction strength affects order-disorder transition temperature (ODT) stronger attraction=higher ODT. side chain length modulates the spacing between features such as cylinders or lamellae.
mentions atomistic and 1 site model of p3ht. atomistic does not access time scales long enough to observe lamellae formation. 1 site model couldn't see lamellae probably because there are no side chains.
The CG model parameter are obtained in ref 29 added below. LJ potential is used for nonbonded. Harmonic bonding used.

\cite{Schwarz2013}
read this with the intention of understanding how the CG potentials were derived.
OK I don't know what Langevin dynamics are but I think it means modelling molecular systems with reduced degrees of freedom and accounting for omitted dof using random kicks to the system?? Mimics solvent or atmospheric effects.
CG model matches experimental phase behaviour. 
ooh really nice figure showing the different length scales (diffraction peaks) in p3ht
diffraction can't resolve molecular level interactions.
this paper describes p3ht nanostructure formation in solution
model: equilibrium bond lengths, angles, charges from DFT. Based on OPLSAA. model agrees with density crystal structure, heat of sublimation. CG FF determined using IBI.
Simulations appear really low density compared to what I'm used to seeing.

\cite{Cummings2019}

each research group used to maintain their own code ,but now these codes are general purpose and shared. open source aid reproducibility because (1) users can look at the source code to find bug and these can be very quickly fixed, (2) monetary cost can exclude users who can't afford it.
a disadvantage is that open source software may be less new user friendly and instead of a gui may use scripts. scripts are an advantage to reproducability however because they allow for user to show and version control exactly what they did with little to no ambiguity. GUI's may obscure the actual mehtod.
ooh I want to look at etomica. hmm. doesnt work in my browser.
overall this was a good summary of what exists but no big takeaways

\cite{Zhang2008}

next time read this and write about mapping operators
essential dynamics coarse-graining. replacing large and complex domains of biomolecules with relatively few CG sites. CG map variationally determined to best reflect the essential dynamics. CG model designed to explore protein conformational space.
processes tht occur on length and time sclaes far beyond simulations with atomic resolution (virus capsid assembly,etc) 
two facets -- number of CG sites and location. for lipids or peptides it is reasonable to replace each functional group with a bead. different levels of resolution for each amino acid. but when looking at HIV capsid composed of thousand of CA protein dimers each CA composed of >200 residues we need a few CG sites per dimer. actin filaments are a similar problem. 
other solutions: 
- (shape focused) rigid units defined by a topological algorithm (like a hard polygon?)  
- (movement focused) CG map that reflects collective motions determined by principle component analysis. small number of PCA modes which define essential dynamics. basically this article is about choosing sites that will best reflect the movements of a protein using math.


\cite{Cao2015a}
if the properties you are interested in a electrostatic, then a CG mapping operator based on centers of charge in the molecule should be best.
mapping operators are chosen based on desired resolution and the information to be extracted. straightforward way is to group three or four heavy atoms together. In the MARTINI force field four types of atoms are grouped and the bead is classified as polar, non-polar, apolar, or charged. 
use chemical intuition--e.g. what parts of the molecule shouldn't be allowed to bend?

\cite{li2020}
excited to read this! Andrew White was Rainier's advisor.


\bibliography{library}
\bibliographystyle{ieeetr}
\end{document}

